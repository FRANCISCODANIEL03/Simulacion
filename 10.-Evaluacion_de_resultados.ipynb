{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1162b4db-bee2-4f6f-af1d-7365360b9eaa",
   "metadata": {},
   "source": [
    "# Evaluacion de resultados\n",
    "\n",
    "En este Notebook se muestran para la evaluacion de los resultados de una prediccion con un algoritmo de Machine Learning\n",
    "\n",
    "## Dataset\n",
    "\n",
    "### Descripción \n",
    "NSL-KDD is a data set suggested to solve some of the inherent problems of the KDD'99 data set which are mentioned in. Although, this new version of the KDD data set still suffers from some of the problems discussed by McHugh and may not be a perfect representative of existing real networks, because of the lack of public data sets for network-based IDSs, we believe it still can be applied as an effective benchmark data set to help researchers compare different intrusion detection methods. Furthermore, the number of records in the NSL-KDD train and test sets are reasonable. This advantage makes it affordable to run the experiments on the complete set without the need to randomly select a small portion. Consequently, evaluation results of different research work will be consistent and comparable.\n",
    "\n",
    "## Ficheros de datos\n",
    "* <span style=\"color:green\">**KDDTrain+.ARFF**: The full NSL-KDD train set with binary labels in ARFF format</span>\n",
    "* KDDTrain+.TXT: The full NSL-KDD train set including attack-type labels and difficulty level in CSV format\n",
    "* KDDTrain+_20Percent.ARFF: A 20% subset of the KDDTrain+.arff file\n",
    "* KDDTrain+_20Percent.TXT: A 20% subset of the KDDTrain+.txt file\n",
    "* KDDTest+.ARFF: The full NSL-KDD test set with binary labels in ARFF format\n",
    "* KDDTest+.TXT: The full NSL-KDD test set including attack-type labels and difficulty level in CSV format\n",
    "* KDDTest-21.ARFF: A subset of the KDDTest+.arff file which does not include records with difficulty level of 21 out of 21\n",
    "* KDDTest-21.TXT: A subset of the KDDTest+.txt file which does not include records with difficulty level of 21 out of 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5753e-7c7f-45fd-8105-3f68db0332a3",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04845fe7-3b97-449f-8e35-abd2d554d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a4ab5-77f3-44bc-8965-2e8d5df5e732",
   "metadata": {},
   "source": [
    "## Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6748e43b-b5e2-41ad-ba46-f9e45d54a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kdd_dataset(data_path):\n",
    "    \"\"\"Lectura de DataSet NSL-KDD.\"\"\"\n",
    "    with open(data_path, 'r') as train_set:\n",
    "        dataset = arff.load(train_set)\n",
    "    attributes = [attr[0] for attr in dataset[\"attributes\"]]\n",
    "    return pd.DataFrame(dataset[\"data\"], columns = attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458110a1-b44f-4a60-8a07-29007c8d9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción de una función que relice le particionado completo\n",
    "def train_val_test_split(df, rstate = 42, shuffle = True, stratify = None):\n",
    "    strat = df[stratify] if stratify else None\n",
    "    train_set, test_set = train_test_split(\n",
    "        df, test_size = 0.4, random_state = rstate, shuffle = shuffle, stratify = strat)\n",
    "    strat = test_set[stratify] if stratify else None\n",
    "    val_set, test_set = train_test_split(\n",
    "        test_set, test_size = 0.5, random_state = rstate, shuffle = shuffle, stratify = strat)\n",
    "    return (train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36dd19b2-23da-41db-8272-095a56724630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construccion de un pipeline para los atributos numericos\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('rbst_scaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933571b9-c1f8-4466-a882-a2ca5d5fe21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador para codificar unicamente las columnas categoricas y devolver un DataFrame\n",
    "class CustomOneHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._oh = OneHotEncoder(sparse_output = False)\n",
    "        self._columns = None\n",
    "    def fit(self, x, y = None):\n",
    "        x_cat = x.select_dtypes(include = [\"object\"])\n",
    "        self._columns = pd.get_dummies(x_cat).columns\n",
    "        self._oh.fit(x_cat)\n",
    "        return self\n",
    "    def transform(self, x, y = None):\n",
    "        x_copy = x.copy()\n",
    "        x_cat = x_copy.select_dtypes(include = [\"object\"])\n",
    "        x_num = x_copy.select_dtypes(exclude = [\"object\"])\n",
    "        x_cat_oh = self._oh.transform(x_cat)\n",
    "        x_cat_oh = pd.DataFrame(x_cat_oh, columns = self._columns, index = x_copy.index)\n",
    "        x_copy.drop(list(x_cat), axis = 1, inplace = True)\n",
    "        return x_copy.join(x_cat_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127bfce8-ed12-48c7-8735-b1c3864507b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador queprepara todo el DataSet llamado Pipelines y transformadores personalizados\n",
    "class DataFramePreparer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._full_pipeline = None\n",
    "        self._columns = None\n",
    "    def fit(self, x, y = None):\n",
    "        num_attribs = list(x.select_dtypes(exclude = ['object']))\n",
    "        cat_attribs = list(x.select_dtypes(include = ['object']))\n",
    "        self._full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attribs),\n",
    "            (\"cat\", CustomOneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "        self._full_pipeline.fit(x)\n",
    "        self._columns = pd.get_dummies(x).columns\n",
    "        return self\n",
    "    def transform(self, x, y = None):\n",
    "        x_copy = x.copy()\n",
    "        x_prep = self._full_pipeline.transform(x_copy)\n",
    "        return pd.DataFrame(x_prep, columns = self._columns, index = x_copy.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78474446-464e-4dce-af3f-0ff739199052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
